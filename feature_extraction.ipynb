{"cells":[{"cell_type":"markdown","metadata":{"id":"vZxY44m0iblX"},"source":["### Importing Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7nBBKrrhibla","executionInfo":{"status":"ok","timestamp":1727514983190,"user_tz":-330,"elapsed":6650,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}}},"outputs":[],"source":["import os\n","import cv2\n","import pandas as pd\n","import cupy as cp  # CuPy for GPU-based NumPy operations\n","import numpy as np\n","import tensorflow as tf\n","import scipy\n","from skimage.feature import local_binary_pattern\n","from skimage.filters import gabor\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"raw","metadata":{"vscode":{"languageId":"raw"},"id":"efHO7J27iblc"},"source":["import os\n","\n","# Set the environment variable\n","os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n"]},{"cell_type":"raw","metadata":{"vscode":{"languageId":"raw"},"id":"lEfXpNK9iblc"},"source":["print(os.getenv('TF_ENABLE_ONEDNN_OPTS'))\n"]},{"cell_type":"code","source":["#!pip install cupy --no-cache-dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0G1NJ9ANB5uW","executionInfo":{"status":"ok","timestamp":1727505182869,"user_tz":-330,"elapsed":30396,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"77eb2403-7287-4282-f6e2-2e5eb6c01131"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting cupy\n","  Downloading cupy-13.3.0.tar.gz (3.4 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m167.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy<2.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from cupy) (1.26.4)\n","Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy) (0.8.2)\n","Building wheels for collected packages: cupy\n","Y\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for cupy (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for cupy\u001b[0m\u001b[31m\n","\u001b[0m\u001b[?25h  Running setup.py clean for cupy\n","Failed to build cupy\n","\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (cupy)\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"markdown","metadata":{"id":"KZSZs8B9ibld"},"source":["### Feature Extraction"]},{"cell_type":"markdown","metadata":{"id":"PAhivoGgibld"},"source":["**Define the paths for the images**"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TaZB5uAxjmQN","executionInfo":{"status":"ok","timestamp":1727515028646,"user_tz":-330,"elapsed":29918,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"85b574c2-9689-4c8c-cdab-aa7b1e59011a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"-qalnLJ4ible","executionInfo":{"status":"ok","timestamp":1727515033742,"user_tz":-330,"elapsed":689,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}}},"outputs":[],"source":["# Define paths\n","dataset_dir = \"/content/drive/My Drive/Fabric Detection Project/textures 3\"\n","categories = ['cotton', 'corduroy', 'denim', 'linin', 'wool']"]},{"cell_type":"markdown","metadata":{"id":"sKetwRX2ible"},"source":["**Canny Edge Detection**"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"4RbvGwD3ible","executionInfo":{"status":"ok","timestamp":1727515035687,"user_tz":-330,"elapsed":995,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}}},"outputs":[],"source":["def extract_canny_edge_detection(image):\n","    \"\"\" image must be passes to the function in grayscale\"\"\"\n","    # Step 1: Enhance contrast (optional)\n","    equalized_image = cp.asarray(cv2.equalizeHist(cp.asnumpy(image)))\n","\n","    # Step 2: Apply Gaussian Blur to reduce noise\n","    blurred_image = cp.asarray(cv2.GaussianBlur(cp.asnumpy(equalized_image), (3, 3), 1))\n","\n","    # Step 3: Apply Canny Edge Detection with adjusted thresholds (convert back and forth)\n","    edges = cp.asarray(cv2.Canny(cp.asnumpy(blurred_image), 30, 30))\n","\n","    return edges"]},{"cell_type":"markdown","metadata":{"id":"H1RIWzDqiblf"},"source":["**Gabor Filtering**"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"p4dwvy1iiblf","executionInfo":{"status":"ok","timestamp":1727515037360,"user_tz":-330,"elapsed":1,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}}},"outputs":[],"source":["def extract_gabor_filters(image):\n","    \"\"\" image must be in grayscale\"\"\"\n","\n","    def build_kernels():\n","        # Parameters\n","        gabor_kernels = []\n","        angles = [0, cp.pi/4, cp.pi/2, 3*cp.pi/4]  # Use CuPy for angles\n","        ksize = 31  # Size of the filter\n","        sigma = 4.0  # Standard deviation of the Gaussian envelope\n","        lambd = 10.0  # Wavelength of the sinusoidal factor\n","        gamma = 0.5  # Spatial aspect ratio\n","        psi = 0  # Phase offset\n","\n","        # Create Gabor kernels\n","        for theta in np.deg2rad([45, 135]):  # Convert degrees to radians\n","            kernel = cp.asarray(cv2.getGaborKernel((ksize, ksize), sigma, theta, lambd, gamma, psi, ktype=cv2.CV_32F)) # Using Cupy array\n","            gabor_kernels.append(kernel)\n","\n","        return gabor_kernels\n","\n","\n","    gabor_kernels = build_kernels()\n","\n","    gabor_features = []\n","\n","    for kernel in gabor_kernels:\n","        fimg = cp.asarray(cv2.filter2D(cp.asnumpy(image), cv2.CV_8UC3, cp.asnumpy(kernel)))\n","        gabor_features.append(fimg)\n","\n","    gabor_features = cp.array(gabor_features).flatten()\n","\n","    return gabor_features"]},{"cell_type":"markdown","metadata":{"id":"0Xfgbxs7iblf"},"source":["**Local Binary Pattern**"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"x-d_j0Dwiblf","executionInfo":{"status":"ok","timestamp":1727515038207,"user_tz":-330,"elapsed":2,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}}},"outputs":[],"source":["def extract_local_binary_pattern(image):\n","\n","    # Parameters\n","    radius = 1\n","    n_points = 8 * radius\n","\n","\n","    lbp = local_binary_pattern(cp.asnumpy(image), n_points, radius, method=\"uniform\")\n","    (hist, _) = cp.histogram(cp.asarray(lbp).ravel(), bins=cp.arange(0, n_points + 3),\n","                             range=(0, n_points + 2))\n","    hist = hist.astype(\"float\")\n","    hist /= (hist.sum() + 1e-6)\n","\n","    return hist"]},{"cell_type":"markdown","metadata":{"id":"nA2JULNriblg"},"source":["**Feature Extraction**"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"5CQjeI6Piblg","executionInfo":{"status":"ok","timestamp":1727515041813,"user_tz":-330,"elapsed":827,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}}},"outputs":[],"source":["# Function to extract features from an image\n","def extract_features(image):\n","    # Convert to grayscale using CuPy arrays\n","    gray = cp.asarray(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))\n","\n","    # Canny edge detection\n","    edges = extract_canny_edge_detection(gray)\n","\n","    # Gabor Filter responses\n","    gabor_features=  extract_gabor_filters(gray)\n","\n","    # Local Binary Patterns (LBP)\n","    hist = extract_local_binary_pattern(gray)\n","\n","    # Combine features: edges, Gabor, and LBP\n","    features = cp.hstack([edges.flatten(), gabor_features, hist])\n","    features = cp.asnumpy(features)\n","\n","    return features # Return features back as NumPy array for further processing"]},{"cell_type":"markdown","metadata":{"id":"DR6Lx77fiblg"},"source":["**Image Augmentation**"]},{"cell_type":"raw","metadata":{"vscode":{"languageId":"raw"},"id":"i5zdvnx3iblg"},"source":["# Prepare dataset and labels\n","data = []\n","labels = []\n","\n","# Image Augmentation using TensorFlow\n","datagen = ImageDataGenerator(\n","    #rotation_range=15,        # Random rotations up to 15 degrees\n","    #width_shift_range=0.1,    # Horizontal shifts\n","    #height_shift_range=0.1,   # Vertical shifts\n","    horizontal_flip=True,     # Flip images horizontally\n","    vertical_flip=True,       # Flip images vertically\n","    #zoom_range=0.2,           # Random zoom\n","    brightness_range=[0.8, 1.2], # Brightness adjustment\n","    shear_range=0.4           # Shear transformation\n",")\n","\n","for num,category in enumerate(categories):\n","    path = os.path.join(dataset_dir, category)\n","    label = category\n","\n","    for count,img_name in enumerate(os.listdir(path),start=1):\n","\n","        if count==2:\n","            print(f\"Shape of the features: {features.shape}\")\n","\n","\n","        img_path = os.path.join(path, img_name)\n","        image = cv2.imread(img_path)\n","\n","        try:\n","            # Apply data augmentation and extract features\n","            image = cv2.resize(image, (128, 128))  # Resize to a fixed size\n","            image = np.expand_dims(image, axis=0)  # Prepare for augmentation\n","            aug_iter = datagen.flow(image, batch_size=1)\n","\n","            # Perform 4 augmentations per image\n","            for _ in range(4):\n","                aug_img = next(aug_iter)[0].astype(np.uint8)\n","                features = extract_features(aug_img)\n","                data.append(features)\n","                labels.append(label)\n","\n","\n","        except Exception as e:\n","            print(img_path,img_name)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-Obsdnw9iblh"},"source":["### Modified code for grouping of images and also including the original image with its augmentations. Also creating only 4 augmentations of each original image"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CS1U3Bf5iblh","executionInfo":{"status":"ok","timestamp":1727515407320,"user_tz":-330,"elapsed":358283,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"87a078ff-56e0-4ed8-80ea-5c136927d265"},"outputs":[{"output_type":"stream","name":"stdout","text":["OpenCV(4.10.0) /io/opencv/modules/imgproc/src/resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"," Error processing image: /content/drive/My Drive/Fabric Detection Project/textures 3/corduroy/.ipynb_checkpoints, .ipynb_checkpoints\n","\n"]}],"source":["# Prepare dataset, labels, and groups\n","X = []\n","y = []\n","groups = []  # This will store the group IDs\n","\n","# Image Augmentation using TensorFlow\n","datagen = ImageDataGenerator(\n","    horizontal_flip=True,\n","    vertical_flip=True,\n","    brightness_range=[0.8, 1.2],\n","    shear_range=0.4\n",")\n","\n","group_id = 0  # Initialize group ID\n","\n","for category in categories:\n","    path = os.path.join(dataset_dir, category)\n","    label = category\n","\n","    for count, img_name in enumerate(os.listdir(path), start=1):\n","\n","        img_path = os.path.join(path, img_name)\n","        image = cv2.imread(img_path)\n","\n","        try:\n","            # Apply data augmentation and extract features\n","            image = cv2.resize(image, (128, 128))  # Resize to a fixed size\n","\n","            # Extract features from the original image\n","            original_features = extract_features(image)\n","            X.append(original_features)\n","            y.append(label)\n","            groups.append(group_id)  # Assign the group ID to the original image\n","\n","            # Prepare for augmentation\n","            image = np.expand_dims(image, axis=0)\n","            aug_iter = datagen.flow(image, batch_size=1)\n","\n","            # Perform 4 augmentations per image\n","            for _ in range(4):\n","                aug_img = next(aug_iter)[0].astype(np.uint8)\n","                features = extract_features(aug_img)\n","                X.append(features)\n","                y.append(label)\n","                groups.append(group_id)  # Assign the same group ID to the augmentations\n","\n","            # Increment group ID for the next image and its augmentations\n","            group_id += 1\n","\n","        except Exception as e:\n","            print(f\"{e} Error processing image: {img_path}, {img_name}\")\n","\n","\n","print()"]},{"cell_type":"markdown","source":["## Feature Engineering"],"metadata":{"id":"SiNB3t2jUh3D"}},{"cell_type":"markdown","source":["**Converting X, y, groups from lists to ndarrays**"],"metadata":{"id":"9iKm3yaAUlXz"}},{"cell_type":"code","source":["# Checking the data type of X,y and groups\n","print(f\"Type of X {type(X)}\")\n","print(f\"Type of y {type(y)}\")\n","print(f\"Type of groups {type(groups)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xM9pSuemSgPz","executionInfo":{"status":"ok","timestamp":1727515412447,"user_tz":-330,"elapsed":645,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"3ebe8795-0277-4ae2-c7b5-968ad4462445"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Type of X <class 'list'>\n","Type of y <class 'list'>\n","Type of groups <class 'list'>\n"]}]},{"cell_type":"code","source":["# Convert lists to NumPy arrays\n","X = np.array(X)\n","y = np.array(y)\n","groups = np.array(groups)"],"metadata":{"id":"8f43V0hdSnNm","executionInfo":{"status":"ok","timestamp":1727515416427,"user_tz":-330,"elapsed":1252,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Checking the data type of X,y and groups\n","print(f\"Type of X {type(X)}\")\n","print(f\"Type of y {type(y)}\")\n","print(f\"Type of groups {type(groups)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EFGRP_1sTFHC","executionInfo":{"status":"ok","timestamp":1727515416427,"user_tz":-330,"elapsed":6,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"1f906864-2ead-47be-d294-c972e39a1156"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Type of X <class 'numpy.ndarray'>\n","Type of y <class 'numpy.ndarray'>\n","Type of groups <class 'numpy.ndarray'>\n"]}]},{"cell_type":"markdown","source":["**Shape of X, y, groups**"],"metadata":{"id":"XAhuPuzuUtGh"}},{"cell_type":"code","source":["# Checking the shape of the dataset and the labels\n","print(f\"Dataset shape: {X.shape}\")\n","print(f\"Labels shape: {y.shape}\")\n","print(f\"Groups shape: {groups.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T87MYemSSpSb","executionInfo":{"status":"ok","timestamp":1727515421379,"user_tz":-330,"elapsed":739,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"7d44507b-c2f4-4430-9a42-2d52460c1034"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset shape: (17875, 49162)\n","Labels shape: (17875,)\n","Groups shape: (17875,)\n"]}]},{"cell_type":"markdown","source":["### Inspecting the dtypes to save memory"],"metadata":{"id":"Y70cNaiMUwx6"}},{"cell_type":"code","source":["print(f\"Dtype of X {X.dtype}\")\n","print(f\"Dtype of y {y.dtype}\")\n","print(f\"Dtype of groups {groups.dtype}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pn9PzzcIS73h","executionInfo":{"status":"ok","timestamp":1727515425689,"user_tz":-330,"elapsed":669,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"25ed96a0-58c9-4bd6-a127-b93efeb20eee"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Dtype of X float64\n","Dtype of y <U8\n","Dtype of groups int64\n"]}]},{"cell_type":"markdown","source":["**Inspecting the dtype of X**"],"metadata":{"id":"2zZL7IeRU2Za"}},{"cell_type":"code","source":["print(f\"Dtype of X {X.dtype}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pTSe0T5vU9m4","executionInfo":{"status":"ok","timestamp":1727515429781,"user_tz":-330,"elapsed":660,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"b5def42e-d611-4c37-c443-75c463da5b66"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Dtype of X float64\n"]}]},{"cell_type":"code","source":["# Checking the size of X in GB\n","print(f\"Size(GB) of X {X.nbytes/1e9}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CJrE5YobTJGk","executionInfo":{"status":"ok","timestamp":1727515432893,"user_tz":-330,"elapsed":5,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"0a8684a8-c74c-4fe7-a306-33f5ae9bd2e6"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Size(GB) of X 7.030166\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Check if any values have non-zero decimals\n","has_decimals = np.any(X != np.floor(X))\n","\n","if not has_decimals:\n","    # Convert the array to int32 if no decimals\n","    X = X.astype(np.int32)\n","    print(\"Array converted to int32 without decimals.\")\n","else:\n","    print(\"Array contains non-zero decimals.\")\n","\n","print(f\"Size(GB) of X {X.nbytes/1e9}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dwyWMAIPTLFC","executionInfo":{"status":"ok","timestamp":1727515440746,"user_tz":-330,"elapsed":3763,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"3052f4f6-76f3-4971-9e36-4ed3a5b18ad7"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Array contains non-zero decimals.\n","Size(GB) of X 7.030166\n"]}]},{"cell_type":"code","source":["len(np.unique(X))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4PdUIf4CTOLQ","executionInfo":{"status":"ok","timestamp":1727515473983,"user_tz":-330,"elapsed":25379,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"3721bb6c-df4f-420e-c020-d85bee6c3d0d"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5840"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# Check if values exceed float16 limits\n","float16_min = np.finfo(np.float16).min\n","float16_max = np.finfo(np.float16).max\n","\n","# Check if any value is outside the float32 range\n","if np.any(X < float16_min) or np.any(X > float16_max):\n","    print(\"Array contains values outside the float16 range.\")\n","else:\n","    print(\"All values are within the float16 range.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"46WbEqbnTO8C","executionInfo":{"status":"ok","timestamp":1727515487985,"user_tz":-330,"elapsed":3226,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"7b7e9153-2017-4a19-8371-9b4c71db69a1"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["All values are within the float16 range.\n"]}]},{"cell_type":"code","source":["# Converting X from float64 to float16\n","# Checking the size of X in GB\n","print(f\"Size(GB) of X when float64 {X.nbytes/1e9}\")\n","\n","X = X.astype(np.float16)\n","\n","# Checking the size of X in GB\n","print(f\"Size(GB) of X after converting to float16 {X.nbytes/1e9}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kWtTzGjITTt6","executionInfo":{"status":"ok","timestamp":1727515498953,"user_tz":-330,"elapsed":6083,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"90ce0b0b-3435-41a7-cb5a-8e097cf9e942"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Size(GB) of X when float64 7.030166\n","Size(GB) of X after converting to float16 1.7575415\n"]}]},{"cell_type":"markdown","source":["**Inspecting the dtype of y**"],"metadata":{"id":"GGk-MOwcWcoc"}},{"cell_type":"markdown","source":["1. Encoding y"],"metadata":{"id":"xE9HFiELWkUJ"}},{"cell_type":"code","source":["# Example array with categories\n","categories = ['linen', 'cotton', 'wool', 'denim', 'corduroy']\n","\n","# Create a dictionary for manual mapping\n","category_mapping = { 'corduroy': 1, 'cotton': 2, 'denim': 3, 'linin': 4, 'wool': 5}\n","\n","# Convert to pandas Series (optional if already in pandas)\n","y_series = pd.Series(y)\n","\n","# Map categories to numbers\n","mapped_categories = y_series.map(category_mapping)\n","y = np.array(mapped_categories)\n","\n","\n","print(y)\n","print(y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GR9S7NZhSq8a","executionInfo":{"status":"ok","timestamp":1727515519315,"user_tz":-330,"elapsed":936,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"4c17d131-6324-4507-f052-b0bda32959eb"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["[2 2 2 ... 5 5 5]\n","(17875,)\n"]}]},{"cell_type":"code","source":["# Freeing up memory\n","del y_series\n","del mapped_categories"],"metadata":{"id":"5MsgDz3vWuJZ","executionInfo":{"status":"ok","timestamp":1727515523038,"user_tz":-330,"elapsed":3,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["2. Checking the dtye of y"],"metadata":{"id":"L10U1FXjWo5C"}},{"cell_type":"code","source":["print(f\"Dtype of y {y.dtype}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YgMA7YsAS1nG","executionInfo":{"status":"ok","timestamp":1727515530035,"user_tz":-330,"elapsed":689,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"b0a83c52-b3a4-4373-a0ba-e04e96ad0bf0"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Dtype of y int64\n"]}]},{"cell_type":"code","source":["print(f\"Size(GB) of y {y.nbytes/1e9}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M7DzX1pXW5Vw","executionInfo":{"status":"ok","timestamp":1727515532621,"user_tz":-330,"elapsed":6,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"9eab9ec3-48a9-422f-db02-9acc45543b91"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Size(GB) of y 0.000143\n"]}]},{"cell_type":"code","source":["print(np.unique(y))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N9e1L5o0WX8g","executionInfo":{"status":"ok","timestamp":1727515535168,"user_tz":-330,"elapsed":6,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"346475dc-b89c-4099-ed5f-5b4e3da5ea18"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 2 3 4 5]\n"]}]},{"cell_type":"code","source":["# Converting the dtype of y to uint8\n","y = y.astype(np.uint8)\n","\n","print(f\"Dtype of y {y.dtype}\")\n","print(f\"Size(GB) of y {y.nbytes/1e9}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tFAt6hgjWX5r","executionInfo":{"status":"ok","timestamp":1727515537879,"user_tz":-330,"elapsed":5,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"11732a72-7b4c-4fa4-827b-b9a376b263f8"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Dtype of y uint8\n","Size(GB) of y 1.7875e-05\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"XR_J-yAEWX22"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Inspecting the dtype of groups**"],"metadata":{"id":"_rP6hqwoXwon"}},{"cell_type":"code","source":["print(f\"Dtype of groups {groups.dtype}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SqgyWnIVXz0I","executionInfo":{"status":"ok","timestamp":1727515552501,"user_tz":-330,"elapsed":720,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"ceee644b-7380-4f2b-bb79-fee21e16169c"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Dtype of groups int64\n"]}]},{"cell_type":"code","source":["# Get the min and max values for int8\n","int8_min = np.iinfo(np.int8).min\n","int8_max = np.iinfo(np.int8).max\n","\n","# Check if all values fall within the int8 range\n","if np.all((groups >= int8_min) & (groups <= int8_max)):\n","    print(\"All values fall within the int8 range.\")\n","else:\n","    print(\"Some values are outside the int8 range.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aEFciUXk8735","executionInfo":{"status":"ok","timestamp":1727515617428,"user_tz":-330,"elapsed":1969,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"62198f5d-a1da-4aac-9beb-b0b7d2a65860"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Some values are outside the int8 range.\n"]}]},{"cell_type":"code","source":["# Get the min and max values for int16\n","int16_min = np.iinfo(np.int16).min\n","int16_max = np.iinfo(np.int16).max\n","\n","# Check if all values fall within the int16 range\n","if np.all((groups >= int16_min) & (groups <= int16_max)):\n","    print(\"All values fall within the int16 range.\")\n","else:\n","    print(\"Some values are outside the int16 range.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r5mD650VYlg6","executionInfo":{"status":"ok","timestamp":1727515624656,"user_tz":-330,"elapsed":2940,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"cbcce35e-de6e-4b44-fc71-28175de94731"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["All values fall within the int16 range.\n"]}]},{"cell_type":"code","source":["# Changing the dtype of groups into int32\n","print(f\"Size of groups when int64 is {groups.nbytes/1e9}\")\n","groups = groups.astype(np.int16)\n","print(f\"Size of groups after changing to int16 is {groups.nbytes/1e9}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OidaFcg6WX0P","executionInfo":{"status":"ok","timestamp":1727515630142,"user_tz":-330,"elapsed":664,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"97f9e74b-f30f-4d90-8060-74fdd534fe16"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of groups when int64 is 0.000143\n","Size of groups after changing to int16 is 3.575e-05\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ge5tiv-fXrdl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jdJfUBXVXra5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3mzsYdr5XrX7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-ph9BIimXrSN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"D6RpWJj-XrPR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xb772p5Bibli","executionInfo":{"status":"ok","timestamp":1727515652929,"user_tz":-330,"elapsed":5249,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"9657cff8-296d-41e2-86b0-23c7fd34d37d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data, labels, and groups have been saved to /content/drive/My Drive/Fabric Detection Project/Extracted Features\n"]}],"source":["\n","# Path to the 'Extracted Features' directory\n","save_dir = \"/content/drive/My Drive/Fabric Detection Project/Extracted Features\"\n","\n","# Create 'Extracted Features' directory if it doesn't exist\n","if not os.path.exists(save_dir):\n","    os.makedirs(save_dir)\n","\n","# Save each array as a separate .npz file\n","np.savez(os.path.join(save_dir, 'X.npz'), data=X)\n","np.savez(os.path.join(save_dir, 'y.npz'), labels=y)\n","np.savez(os.path.join(save_dir, 'groups.npz'), groups=groups)\n","\n","print(f\"Data, labels, and groups have been saved to {save_dir}\")\n"]},{"cell_type":"code","source":["# Estimate the size of each array in gigabytes (in memory)\n","import sys\n","data_size = sys.getsizeof(X) / (1024 * 1024 * 1024)\n","labels_size = sys.getsizeof(y) / (1024 * 1024 * 1024)\n","groups_size = sys.getsizeof(groups) / (1024 * 1024 * 1024)\n","\n","print(f\"Size of the data.npz :{data_size}\")\n","print(f\"Size of the labels.npz :{labels_size}\")\n","print(f\"Size of the groups.npz :{groups_size}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QtzqMnEcsfoI","executionInfo":{"status":"ok","timestamp":1727515642329,"user_tz":-330,"elapsed":1207,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"abdd5bee-07fa-4f3b-b295-12a75ba304e0"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of the data.npz :1.6368381939828396\n","Size of the labels.npz :1.6751699149608612e-05\n","Size of the groups.npz :3.339909017086029e-05\n"]}]},{"cell_type":"markdown","metadata":{"id":"RzNV-pSMibli"},"source":["### New CODE FOR DATA AUGMENTATION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bsnLImC5iblj","outputId":"c06884b4-bd26-4eb6-f204-8885616c2899"},"outputs":[{"name":"stdout","output_type":"stream","text":["Error processing image: textures 3/corduroy/.ipynb_checkpoints, .ipynb_checkpoints\n"]}],"source":["import numpy as np\n","import os\n","import cv2\n","import gc\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Initialize batch size for saving\n","batch_size = 1000  # You can adjust this based on memory capacity\n","save_count = 0  # To track saved files\n","\n","# Initialize arrays\n","data = []\n","labels = []\n","groups = []  # This will store the group IDs\n","\n","group_id = 0  # Initialize group ID\n","\n","# Image Augmentation using TensorFlow\n","datagen = ImageDataGenerator(\n","    horizontal_flip=True,\n","    vertical_flip=True,\n","    brightness_range=[0.8, 1.2],\n","    shear_range=0.4\n",")\n","\n","for category in categories:\n","    path = os.path.join(dataset_dir, category)\n","    label = category\n","\n","    for count, img_name in enumerate(os.listdir(path), start=1):\n","\n","        img_path = os.path.join(path, img_name)\n","        image = cv2.imread(img_path)\n","\n","        try:\n","            # Apply data augmentation and extract features\n","            image = cv2.resize(image, (128, 128))  # Resize to a fixed size\n","\n","            # Extract features from the original image\n","            original_features = extract_features(image)\n","            data.append(original_features)\n","            labels.append(label)\n","            groups.append(group_id)  # Assign the group ID to the original image\n","\n","            # Prepare for augmentation\n","            image = np.expand_dims(image, axis=0)\n","            aug_iter = datagen.flow(image, batch_size=1)\n","\n","            # Perform 4 augmentations per image\n","            for _ in range(4):\n","                aug_img = next(aug_iter)[0].astype(np.uint8)\n","                features = extract_features(aug_img)\n","                data.append(features)\n","                labels.append(label)\n","                groups.append(group_id)  # Assign the same group ID to the augmentations\n","\n","            # Increment group ID for the next image and its augmentations\n","            group_id += 1\n","\n","            # Save data in batches\n","            if len(data) >= batch_size:\n","                # Save current batch to an .npz file\n","                np.savez_compressed(f'Extracted_features\\\\dataset_batch_{save_count}.npz', data=np.array(data), labels=np.array(labels), groups=np.array(groups))\n","                save_count += 1\n","\n","                # Clear memory by resetting the arrays and forcing garbage collection\n","                data.clear()\n","                labels.clear()\n","                groups.clear()\n","                gc.collect()  # Force garbage collection to free memory\n","\n","        except Exception as e:\n","            print(f\"Error processing image: {img_path}, {img_name}\")\n","\n","# Save any remaining data if exists after loop ends\n","if data:\n","    np.savez_compressed(f'Extracted_features\\\\dataset_batch_{save_count}.npz', data=np.array(data), labels=np.array(labels), groups=np.array(groups))\n","    data.clear()\n","    labels.clear()\n","    groups.clear()\n","    gc.collect()  # Clean up the remaining memory\n"]},{"cell_type":"markdown","metadata":{"id":"rWJsgyW8iblj"},"source":["### New Code (using data,labels and groups directly as ndarrays rather than lists)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1EpE0NqKiblj","executionInfo":{"status":"ok","timestamp":1727412402536,"user_tz":-330,"elapsed":69760,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"61cc5fbf-bedb-435a-ca42-defc0d91a774"},"outputs":[{"output_type":"stream","name":"stdout","text":["Error processing image: /content/drive/My Drive/Fabric Detection Project/textures 3/corduroy/.ipynb_checkpoints, .ipynb_checkpoints\n","Total original images: 3575\n","Total augmented images: 14300\n","Total images including augmentations: 17875\n","Number of features per image: 49162\n"]}],"source":["\n","# Counting the number of rows(images) and features\n","import os\n","\n","# Initialize counters\n","total_original_images = 0\n","total_augmented_images = 0\n","augmentation_per_image = 4  # As you're doing 4 augmentations per image\n","feature_size = None  # To store the size of features (columns)\n","\n","for category in categories:\n","    path = os.path.join(dataset_dir, category)\n","\n","    for count, img_name in enumerate(os.listdir(path), start=1):\n","        img_path = os.path.join(path, img_name)\n","        image = cv2.imread(img_path)\n","\n","        try:\n","            image = cv2.resize(image, (128, 128))  # Resize to a fixed size\n","            original_features = extract_features(image)\n","\n","            # Determine feature size (columns) from the first image processed\n","            if feature_size is None:\n","                feature_size = original_features.shape[0]  # Assuming 1D feature vector\n","\n","            # Increment counts for original and augmented images\n","            total_original_images += 1\n","            total_augmented_images += augmentation_per_image\n","\n","        except Exception as e:\n","            print(f\"Error processing image: {img_path}, {img_name}\")\n","\n","# Total number of images = original + augmented\n","total_images = total_original_images + total_augmented_images\n","\n","# Print the results\n","print(f\"Total original images: {total_original_images}\")\n","print(f\"Total augmented images: {total_augmented_images}\")\n","print(f\"Total images including augmentations: {total_images}\")\n","print(f\"Number of features per image: {feature_size}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3UzQEf0Xiblj","executionInfo":{"status":"ok","timestamp":1727412785053,"user_tz":-330,"elapsed":247959,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"a517c7e9-cd37-4b8a-e741-3ee408eb936c"},"outputs":[{"output_type":"stream","name":"stdout","text":["OpenCV(4.10.0) /io/opencv/modules/imgproc/src/resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"," /content/drive/My Drive/Fabric Detection Project/textures 3/corduroy/.ipynb_checkpoints,  .ipynb_checkpoints\n"]}],"source":["# Image Augmentation using TensorFlow\n","datagen = ImageDataGenerator(\n","    horizontal_flip=True,\n","    vertical_flip=True,\n","    brightness_range=[0.8, 1.2],\n","    shear_range=0.4\n",")\n","\n","# Step 1: Create \"Extracted_features\" folder if it doesn't exist\n","output_dir = '/content/drive/My Drive/Fabric Detection Project/Extracted_features'\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","    print(f\"Created directory: {output_dir}\")\n","\n","# Step 2: Define file paths for the memory-mapped arrays inside the new folder\n","data_file = os.path.join(output_dir, 'data_file.dat')\n","labels_file = os.path.join(output_dir, 'labels_file.dat')\n","groups_file = os.path.join(output_dir, 'groups_file.dat')\n","# Known values\n","total_images = total_images  # Total original images + augmentations\n","feature_size = feature_size   # Number of features per image\n","\n","# Step 1: Preallocate memory-mapped arrays\n","data = np.memmap(data_file, dtype='float32', mode='w+', shape=(total_images, feature_size))\n","labels = np.memmap(labels_file, dtype='object', mode='w+', shape=(total_images,))\n","groups = np.memmap(groups_file, dtype='int32', mode='w+', shape=(total_images,))\n","\n","# Step 2: Feature Extraction with memory mapping\n","group_id = 0  # Initialize group ID\n","image_counter = 0  # Keep track of which row we are filling in the arrays\n","\n","for category in categories:\n","    path = os.path.join(dataset_dir, category)\n","    label = category\n","\n","    for count, img_name in enumerate(os.listdir(path), start=1):\n","        img_path = os.path.join(path, img_name)\n","        image = cv2.imread(img_path)\n","\n","        try:\n","            image = cv2.resize(image, (128, 128))  # Resize to a fixed size\n","\n","            # Extract features from the original image\n","            original_features = extract_features(image)\n","\n","            # Store the original image's features, label, and group ID in the memory-mapped arrays\n","            data[image_counter, :] = original_features  # Fill in the row corresponding to the current image\n","            labels[image_counter] = label\n","            groups[image_counter] = group_id\n","            image_counter += 1  # Increment counter to fill the next row\n","\n","            # Prepare for augmentation\n","            image = np.expand_dims(image, axis=0)\n","            aug_iter = datagen.flow(image, batch_size=1)\n","\n","            # Perform 4 augmentations per image\n","            for _ in range(4):\n","                aug_img = next(aug_iter)[0].astype(np.uint8)\n","                features = extract_features(aug_img)\n","\n","                # Store augmented image features, label, and group ID\n","                data[image_counter, :] = features\n","                labels[image_counter] = label\n","                groups[image_counter] = group_id\n","                image_counter += 1  # Increment counter for the next augmented image\n","\n","            # Increment group ID for the next set of images\n","            group_id += 1\n","\n","        except Exception as e:\n","            print(e,f\"{img_path},  {img_name}\")\n","\n","# Flush the changes to disk to ensure everything is saved\n","data.flush()\n","labels.flush()\n","groups.flush()\n"]},{"cell_type":"markdown","metadata":{"id":"AM9ROPO1iblk"},"source":["**Freeing up the memory**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrXDPnI3iblk"},"outputs":[],"source":["import gc\n","gc.collect()\n","del data\n","del labels\n","del groups"]},{"cell_type":"markdown","metadata":{"id":"MvVfnxU4iblk"},"source":["**Load the data,labels and groups**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_jD23q7qiblk"},"outputs":[],"source":["import numpy as np\n","import os\n","\n","# Memory-mapped file paths\n","output_dir = '/content/drive/My Drive/Fabric Detection Project/Extracted_features'\n","data_file = os.path.join(output_dir, 'data_file.dat')\n","labels_file = os.path.join(output_dir, 'labels_file.dat')\n","groups_file = os.path.join(output_dir, 'groups_file.dat')\n","\n","# Known values\n","#total_images = 17780  # Total original images + augmentations\n","#feature_size = 49162   # Number of features per image\n","\n","# Load the memory-mapped arrays for reading\n","data = np.memmap(data_file, dtype='float32', mode='r', shape=(total_images, feature_size))\n","labels = np.memmap(labels_file, dtype='object', mode='r', shape=(total_images,))\n","groups = np.memmap(groups_file, dtype='int32', mode='r', shape=(total_images,))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3d4IfVVCiblk","executionInfo":{"status":"ok","timestamp":1727412915627,"user_tz":-330,"elapsed":487,"user":{"displayName":"Parthsarthi Joshi","userId":"13110895326430403805"}},"outputId":"e80b14f4-6009-4760-8458-68a5ea9d46eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["(17780, 49162)\n"]}],"source":["print(data.shape)"]},{"cell_type":"markdown","metadata":{"id":"kvmn0kEAiblk"},"source":["**Data** contains the input data (data) <br>\n","**Labels** contains the output data (Y)"]},{"cell_type":"markdown","metadata":{"id":"6cMhbasTiblk"},"source":[]},{"cell_type":"markdown","metadata":{"id":"9MEqeOBoiblk"},"source":["**Saving the data,labels and groups**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XuwfKBI3ibll","outputId":"e6066c1d-8bf9-4106-fad7-6d8ed7be5a9b"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# Convert lists to NumPy arrays\n","data_np = np.array(data)\n","labels_np = np.array(labels)\n","groups_np = np.array(groups)\n","\n","if not os.path.exists(\"Extracted_features\"):\n","    os.makedirs(\"Extracted_features\")\n","# Save to a .npz file\n","np.savez('Extracted_features\\\\data.npz', data=data_np)\n","np.savez('Extracted_features\\\\labels.npz', labels=labels_np)\n","np.savez('Extracted_features\\\\groups.npz',groups=groups_np)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1pdnbqaLibll","outputId":"882cfcb0-15c0-42ca-a8c4-b8d623772f0f"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["import os\n","import numpy as np\n","\n","# Directory where your .npz files are stored\n","npz_directory = './'  # Replace with the correct directory path if necessary\n","\n","# Automatically list all .npz files in the directory\n","npz_files = [f for f in os.listdir(npz_directory) if f.endswith('.npz')]\n","\n","# Initialize empty lists to store the extracted data\n","all_data = []\n","all_labels = []\n","all_groups = []\n","\n","# Iterate through each saved .npz file and load the arrays\n","for file in npz_files:\n","    file_path = os.path.join(npz_directory, file)\n","    # Load the saved .npz file\n","    with np.load(file_path) as data:\n","        all_data.append(data['data'])    # Append 'data' array\n","        all_labels.append(data['labels']) # Append 'labels' array\n","        all_groups.append(data['groups']) # Append 'groups' array\n","\n","# Now concatenate all arrays to create final datasets\n","final_data = np.concatenate(all_data, axis=0)\n","final_labels = np.concatenate(all_labels, axis=0)\n","final_groups = np.concatenate(all_groups, axis=0)\n","\n","# Your final arrays are now ready to use\n","print(\"Final data shape:\", final_data.shape)\n","print(\"Final labels shape:\", final_labels.shape)\n","print(\"Final groups shape:\", final_groups.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"TlliQf2Yibll"},"source":["--------------------------------End----------------------------------\n","\n","MOVE TO model_training.ipynb"]},{"cell_type":"markdown","metadata":{"id":"qOXi5nBVibll"},"source":["**Loading the saved data and labels**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yRocp5Piblm"},"outputs":[],"source":["# Load from the .npz file\n","loaded_data = np.load('Extracted_features\\\\data.npz')\n","loaded_labels = np.load('Extracted_features\\\\labels.npz')\n","loaded_groups = np.load('Extracted_features\\\\groups.npz')\n","\n","data_loaded = loaded_data[\"data\"]\n","labels_loaded = loaded_labels[\"labels\"]\n","groups_loaded = loaded_labels[\"groups\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YpbZEV4tiblm","outputId":"44a0ab8b-8d0e-4847-d653-c5e87a977363"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset shape: (17780, 49162)\n","Labels shape: (17780,)\n"]}],"source":["# Checking the shape of the dataset and the labels\n","print(f\"Dataset shape: {data_loaded.shape}\")\n","print(f\"Labels shape: {labels_loaded.shape}\")\n","print(f\"Groups shape: {groups_loaded.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"B137FgZLiblq"},"source":["From the above output we can see that we have **49162 features** and **17780 rows**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nlkLsdetiblq"},"outputs":[],"source":["data = data_loaded\n","labels = labels_loaded"]},{"cell_type":"markdown","metadata":{"id":"IHwfwutdiblq"},"source":["### Label Encoding of target variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5o3hi_HMiblq"},"outputs":[],"source":["# Encode labels\n","le = LabelEncoder()\n","labels = le.fit_transform(labels)"]},{"cell_type":"markdown","metadata":{"id":"Xrsazyytiblq"},"source":["### Train-test split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-w2h6Jb2iblq","outputId":"86e58272-1b28-4cd6-f65b-ee14f788243c"},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train shape:  (12446, 49162)\n","X_test shape:  (5334, 49162)\n","y_train shape:  (12446,)\n","y_test shape:  (5334,)\n"]}],"source":["# Split dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(np.array(data), np.array(labels), test_size=0.3, random_state=42)\n","print(\"X_train shape: \",X_train.shape)\n","print(\"X_test shape: \",X_test.shape)\n","print(\"y_train shape: \",y_train.shape)\n","print(\"y_test shape: \",y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"vUIb8n25iblr"},"source":["### Standardization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKO0SqxViblr"},"outputs":[],"source":["scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"Dhq6Ivrkiblr"},"source":["### Dimensionality Reduction using PCA(Principal Component Analysis)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m6mFlPyKiblr"},"outputs":[],"source":["# Dimensionality Reduction using PCA\n","pca = PCA(n_components=1000)\n","X_train = pca.fit_transform(X_train)\n","X_test = pca.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"nCRjKqX-iblr"},"source":["### Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PcgcW3_Fiblr","outputId":"46a69ff5-7173-4345-aa47-8008a2edf962"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: black;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-1 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-1 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-1 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-1 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-1 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-1 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-1 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-1 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: block;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"▸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-1 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"▾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-1 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-1 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-1 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-1 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-1 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 1ex;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-1 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-1 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(random_state=42)</pre></div> </div></div></div></div>"],"text/plain":["RandomForestClassifier(random_state=42)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Train a Random Forest Classifier\n","random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","random_forest_clf.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ZxgtRM1iblr","outputId":"7c1b60ab-8b89-4e2f-a1d9-8d711fc0960c"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-2 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: black;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-2 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-2 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-2 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-2 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-2 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-2 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-2 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-2 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-2 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-2 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-2 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-2 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-2 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-2 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-2 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: block;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","}\n","\n","#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"▸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-2 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-2 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-2 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"▾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-2 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-2 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-2 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-2 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-2 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-2 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-2 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 1ex;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-2 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-2 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-2 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(random_state=42)</pre></div> </div></div></div></div>"],"text/plain":["SVC(random_state=42)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Apply SVM model (using RBF kernel )\n","svm_clf = SVC(kernel='rbf', random_state=42)\n","svm_clf.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"S4ic0yClibls"},"source":["### Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWtzW9u0iblt","outputId":"f605bcf5-e976-4b3c-aabb-5e4b30d1bb4b"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","    corduroy       0.93      0.68      0.79      1055\n","      cotton       0.65      0.89      0.75      1062\n","       denim       0.80      0.94      0.86      1092\n","       linin       0.83      0.61      0.70      1091\n","        wool       0.65      0.63      0.64      1034\n","\n","    accuracy                           0.75      5334\n","   macro avg       0.77      0.75      0.75      5334\n","weighted avg       0.77      0.75      0.75      5334\n","\n","[[ 718   99   78   41  119]\n"," [   8  946   21   31   56]\n"," [   0    2 1023    7   60]\n"," [  21  235   46  668  121]\n"," [  26  180  114   61  653]]\n"]}],"source":["# Predictions and Evaluation\n","y_pred = random_forest_clf.predict(X_test)\n","print(classification_report(y_test, y_pred, target_names=le.classes_))\n","print(confusion_matrix(y_test, y_pred))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2uTZUlEHiblt","outputId":"9e6ecf8a-6e97-468a-e9cc-4fa7bfc74fe9"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","    corduroy       0.52      0.73      0.61      1055\n","      cotton       0.43      0.56      0.48      1062\n","       denim       0.96      0.82      0.88      1092\n","       linin       0.60      0.42      0.49      1091\n","        wool       0.63      0.48      0.55      1034\n","\n","    accuracy                           0.60      5334\n","   macro avg       0.63      0.60      0.60      5334\n","weighted avg       0.63      0.60      0.61      5334\n","\n","[[766 163  11  50  65]\n"," [240 592   5 158  67]\n"," [ 79   8 896  30  79]\n"," [147 395   7 457  85]\n"," [228 225  15  65 501]]\n"]}],"source":["# Predictions and Evaluation\n","y_pred = svm_clf.predict(X_test)\n","print(classification_report(y_test, y_pred, target_names=le.classes_))\n","print(confusion_matrix(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"z69te4oWiblt"},"source":["**Checking class imbalance**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybrKcEgCiblt","outputId":"6c2749e1-d10f-4025-9492-91844f5a99e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["linin       3585\n","denim       3575\n","corduroy    3550\n","cotton      3535\n","wool        3535\n","Name: count, dtype: int64\n"]}],"source":["print(pd.Series(labels_loaded).value_counts())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_a8IIR62iblt"},"outputs":[],"source":["print(len(pd.Series(labels_loaded)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5dEZ6-h8iblt"},"outputs":[],"source":["print((pd.Series(labels_loaded).value_counts() / len(pd.Series(labels_loaded)))*100)\n"]},{"cell_type":"markdown","metadata":{"id":"OmdmAua0iblt"},"source":["Classes are balanced"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xr8wE8XKiblu"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}