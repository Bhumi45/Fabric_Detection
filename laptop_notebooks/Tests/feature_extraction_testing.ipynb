{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cupy as cp  # CuPy for GPU-based NumPy operations\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.filters import gabor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the paths for the images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "dataset_dir = \"textures 3\"\n",
    "categories = ['cotton', 'corduroy', 'denim', 'linin', 'wool']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Canny Edge Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_canny_edge_detection(image):\n",
    "    \"\"\" image must be passes to the function in grayscale\"\"\"\n",
    "    # Step 1: Enhance contrast (optional)\n",
    "    equalized_image = cp.asarray(cv2.equalizeHist(cp.asnumpy(image)))\n",
    "    \n",
    "    # Step 2: Apply Gaussian Blur to reduce noise\n",
    "    blurred_image = cp.asarray(cv2.GaussianBlur(cp.asnumpy(equalized_image), (3, 3), 1))\n",
    "    \n",
    "    # Step 3: Apply Canny Edge Detection with adjusted thresholds (convert back and forth)\n",
    "    edges = cp.asarray(cv2.Canny(cp.asnumpy(blurred_image), 30, 30))\n",
    "\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gabor Filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gabor_filters(image):\n",
    "    \"\"\" image must be in grayscale\"\"\"\n",
    "    \n",
    "    def build_kernels():\n",
    "        # Parameters\n",
    "        gabor_kernels = []\n",
    "        angles = [0, cp.pi/4, cp.pi/2, 3*cp.pi/4]  # Use CuPy for angles\n",
    "        ksize = 31  # Size of the filter\n",
    "        sigma = 4.0  # Standard deviation of the Gaussian envelope\n",
    "        lambd = 10.0  # Wavelength of the sinusoidal factor\n",
    "        gamma = 0.5  # Spatial aspect ratio\n",
    "        psi = 0  # Phase offset\n",
    "\n",
    "        # Create Gabor kernels\n",
    "        for theta in np.deg2rad([45, 135]):  # Convert degrees to radians\n",
    "            kernel = cp.asarray(cv2.getGaborKernel((ksize, ksize), sigma, theta, lambd, gamma, psi, ktype=cv2.CV_32F)) # Using Cupy array\n",
    "            gabor_kernels.append(kernel)\n",
    "\n",
    "        return gabor_kernels\n",
    "\n",
    "\n",
    "    gabor_kernels = build_kernels()\n",
    "    \n",
    "    gabor_features = []\n",
    "\n",
    "    for kernel in gabor_kernels:\n",
    "        fimg = cp.asarray(cv2.filter2D(cp.asnumpy(image), cv2.CV_8UC3, cp.asnumpy(kernel)))\n",
    "        gabor_features.append(fimg)\n",
    "\n",
    "    gabor_features = cp.array(gabor_features).flatten()\n",
    "\n",
    "    return gabor_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local Binary Pattern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_local_binary_pattern(image):\n",
    "\n",
    "    # Parameters \n",
    "    radius = 1\n",
    "    n_points = 8 * radius\n",
    "\n",
    "    \n",
    "    lbp = local_binary_pattern(cp.asnumpy(image), n_points, radius, method=\"uniform\")\n",
    "    (hist, _) = cp.histogram(cp.asarray(lbp).ravel(), bins=cp.arange(0, n_points + 3),\n",
    "                             range=(0, n_points + 2))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-6)\n",
    "\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from an image\n",
    "def extract_features(image):\n",
    "    # Convert to grayscale using CuPy arrays\n",
    "    gray = cp.asarray(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "    # Canny edge detection\n",
    "    edges = extract_canny_edge_detection(gray)\n",
    "    \n",
    "    # Gabor Filter responses\n",
    "    gabor_features=  extract_gabor_filters(gray)\n",
    "    \n",
    "    # Local Binary Patterns (LBP)\n",
    "    hist = extract_local_binary_pattern(gray)\n",
    "    \n",
    "    # Combine features: edges, Gabor, and LBP\n",
    "    features = cp.hstack([edges.flatten(), gabor_features, hist])\n",
    "    features = cp.asnumpy(features)\n",
    "    \n",
    "    return features # Return features back as NumPy array for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Image Augmentation using TensorFlow\n",
    "datagen = ImageDataGenerator(\n",
    "    #rotation_range=15,        # Random rotations up to 15 degrees\n",
    "    #width_shift_range=0.1,    # Horizontal shifts\n",
    "    #height_shift_range=0.1,   # Vertical shifts\n",
    "    horizontal_flip=True,     # Flip images horizontally\n",
    "    vertical_flip=True,       # Flip images vertically\n",
    "    zoom_range=0.2,           # Random zoom\n",
    "    brightness_range=[0.8, 1.2], # Brightness adjustment\n",
    "    shear_range=0.1           # Shear transformation\n",
    ")\n",
    "\n",
    "for category in categories:\n",
    "    path = os.path.join(dataset_dir, category)\n",
    "    label = category\n",
    "    \n",
    "    for count,img_name in enumerate(os.listdir(path),start=1):\n",
    "\n",
    "        if count==30:\n",
    "            break\n",
    "        \n",
    "        img_path = os.path.join(path, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        try:\n",
    "            # Apply data augmentation and extract features\n",
    "            image = cv2.resize(image, (128, 128))  # Resize to a fixed size\n",
    "            image = np.expand_dims(image, axis=0)  # Prepare for augmentation\n",
    "            aug_iter = datagen.flow(image, batch_size=1)\n",
    "\n",
    "            # Perform 5 augmentations per image\n",
    "            for _ in range(5):\n",
    "                aug_img = next(aug_iter)[0].astype(np.uint8)\n",
    "\n",
    "                plt.figure(figsize=(8, 4))\n",
    "\n",
    "                #Original image\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.imshow(image)\n",
    "                plt.title(\"Original Image\")\n",
    "                \n",
    "                \n",
    "                # Augmented image\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.imshow(aug_img)\n",
    "                plt.title(\"Augmented Image\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(img_path,img_name)\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data** contains the input data (X) <br>\n",
    "**Labels** contains the output data (Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the data and labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to NumPy arrays\n",
    "data_np = np.array(data)\n",
    "labels_np = np.array(labels)\n",
    "\n",
    "if not os.path.exists(\"Extracted_features\"):\n",
    "    os.makedirs(\"Extracted_features\")\n",
    "# Save to a .npz file\n",
    "np.savez('Extracted_features\\\\data_labels.npz', data=data_np, labels=labels_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the saved data and labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from the .npz file\n",
    "loaded = np.load('Extracted_features\\\\data_labels.npz')\n",
    "data_loaded = loaded['data']\n",
    "labels_loaded = loaded['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the dataset and the labels\n",
    "print(f\"Dataset shape: {data_loaded.shape}\")\n",
    "print(f\"Labels shape: {labels_loaded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output we can see that we have **49162 features** and **17780 rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_loaded\n",
    "labels = labels_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(data), np.array(labels), test_size=0.3, random_state=42)\n",
    "print(\"X_train shape: \",X_train.shape)\n",
    "print(\"X_test shape: \",X_test.shape)\n",
    "print(\"y_train shape: \",y_train.shape)\n",
    "print(\"y_test shape: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction using PCA(Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction using PCA \n",
    "pca = PCA(n_components=100)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest Classifier \n",
    "random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVM model (using RBF kernel )\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and Evaluation\n",
    "y_pred = random_forest_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and Evaluation\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking class imbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Series(labels_loaded).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pd.Series(labels_loaded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((pd.Series(labels_loaded).value_counts() / len(pd.Series(labels_loaded)))*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
