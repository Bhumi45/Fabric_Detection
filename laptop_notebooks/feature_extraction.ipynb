{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import cupy as cp  # CuPy for GPU-based NumPy operations\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.filters import gabor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "print(os.getenv('TF_ENABLE_ONEDNN_OPTS'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the paths for the images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "dataset_dir = \"textures 3\"\n",
    "categories = ['cotton', 'corduroy', 'denim', 'linin', 'wool']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Canny Edge Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_canny_edge_detection(image):\n",
    "    \"\"\" image must be passes to the function in grayscale\"\"\"\n",
    "    # Step 1: Enhance contrast (optional)\n",
    "    equalized_image = cp.asarray(cv2.equalizeHist(cp.asnumpy(image)))\n",
    "    \n",
    "    # Step 2: Apply Gaussian Blur to reduce noise\n",
    "    blurred_image = cp.asarray(cv2.GaussianBlur(cp.asnumpy(equalized_image), (3, 3), 1))\n",
    "    \n",
    "    # Step 3: Apply Canny Edge Detection with adjusted thresholds (convert back and forth)\n",
    "    edges = cp.asarray(cv2.Canny(cp.asnumpy(blurred_image), 30, 30))\n",
    "\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gabor Filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gabor_filters(image):\n",
    "    \"\"\" image must be in grayscale\"\"\"\n",
    "    \n",
    "    def build_kernels():\n",
    "        # Parameters\n",
    "        gabor_kernels = []\n",
    "        angles = [0, cp.pi/4, cp.pi/2, 3*cp.pi/4]  # Use CuPy for angles\n",
    "        ksize = 31  # Size of the filter\n",
    "        sigma = 4.0  # Standard deviation of the Gaussian envelope\n",
    "        lambd = 10.0  # Wavelength of the sinusoidal factor\n",
    "        gamma = 0.5  # Spatial aspect ratio\n",
    "        psi = 0  # Phase offset\n",
    "\n",
    "        # Create Gabor kernels\n",
    "        for theta in np.deg2rad([45, 135]):  # Convert degrees to radians\n",
    "            kernel = cp.asarray(cv2.getGaborKernel((ksize, ksize), sigma, theta, lambd, gamma, psi, ktype=cv2.CV_32F)) # Using Cupy array\n",
    "            gabor_kernels.append(kernel)\n",
    "\n",
    "        return gabor_kernels\n",
    "\n",
    "\n",
    "    gabor_kernels = build_kernels()\n",
    "    \n",
    "    gabor_features = []\n",
    "\n",
    "    for kernel in gabor_kernels:\n",
    "        fimg = cp.asarray(cv2.filter2D(cp.asnumpy(image), cv2.CV_8UC3, cp.asnumpy(kernel)))\n",
    "        gabor_features.append(fimg)\n",
    "\n",
    "    gabor_features = cp.array(gabor_features).flatten()\n",
    "\n",
    "    return gabor_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local Binary Pattern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_local_binary_pattern(image):\n",
    "\n",
    "    # Parameters \n",
    "    radius = 1\n",
    "    n_points = 8 * radius\n",
    "\n",
    "    \n",
    "    lbp = local_binary_pattern(cp.asnumpy(image), n_points, radius, method=\"uniform\")\n",
    "    (hist, _) = cp.histogram(cp.asarray(lbp).ravel(), bins=cp.arange(0, n_points + 3),\n",
    "                             range=(0, n_points + 2))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-6)\n",
    "\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from an image\n",
    "def extract_features(image):\n",
    "    # Convert to grayscale using CuPy arrays\n",
    "    gray = cp.asarray(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "    # Canny edge detection\n",
    "    edges = extract_canny_edge_detection(gray)\n",
    "    \n",
    "    # Gabor Filter responses\n",
    "    gabor_features=  extract_gabor_filters(gray)\n",
    "    \n",
    "    # Local Binary Patterns (LBP)\n",
    "    hist = extract_local_binary_pattern(gray)\n",
    "    \n",
    "    # Combine features: edges, Gabor, and LBP\n",
    "    features = cp.hstack([edges.flatten(), gabor_features, hist])\n",
    "    features = cp.asnumpy(features)\n",
    "    \n",
    "    return features # Return features back as NumPy array for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Augmentation**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Prepare dataset and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Image Augmentation using TensorFlow\n",
    "datagen = ImageDataGenerator(\n",
    "    #rotation_range=15,        # Random rotations up to 15 degrees\n",
    "    #width_shift_range=0.1,    # Horizontal shifts\n",
    "    #height_shift_range=0.1,   # Vertical shifts\n",
    "    horizontal_flip=True,     # Flip images horizontally\n",
    "    vertical_flip=True,       # Flip images vertically\n",
    "    #zoom_range=0.2,           # Random zoom\n",
    "    brightness_range=[0.8, 1.2], # Brightness adjustment\n",
    "    shear_range=0.4           # Shear transformation\n",
    ")\n",
    "\n",
    "for num,category in enumerate(categories):\n",
    "    path = os.path.join(dataset_dir, category)\n",
    "    label = category\n",
    "    \n",
    "    for count,img_name in enumerate(os.listdir(path),start=1):\n",
    "\n",
    "        if count==2:\n",
    "            print(f\"Shape of the features: {features.shape}\")\n",
    "\n",
    "\n",
    "        img_path = os.path.join(path, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        try:\n",
    "            # Apply data augmentation and extract features\n",
    "            image = cv2.resize(image, (128, 128))  # Resize to a fixed size\n",
    "            image = np.expand_dims(image, axis=0)  # Prepare for augmentation\n",
    "            aug_iter = datagen.flow(image, batch_size=1)\n",
    "\n",
    "            # Perform 4 augmentations per image\n",
    "            for _ in range(4):\n",
    "                aug_img = next(aug_iter)[0].astype(np.uint8)\n",
    "                features = extract_features(aug_img)\n",
    "                data.append(features)\n",
    "                labels.append(label)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(img_path,img_name)\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified code for grouping of images and also including the original image with its augmentations. Also creating only 4 augmentations of each original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image: textures 3/corduroy/.ipynb_checkpoints, .ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset, labels, and groups\n",
    "data = []\n",
    "labels = []\n",
    "groups = []  # This will store the group IDs\n",
    "\n",
    "# Image Augmentation using TensorFlow\n",
    "datagen = ImageDataGenerator(\n",
    "    horizontal_flip=True,     \n",
    "    vertical_flip=True,       \n",
    "    brightness_range=[0.8, 1.2], \n",
    "    shear_range=0.4           \n",
    ")\n",
    "\n",
    "group_id = 0  # Initialize group ID\n",
    "\n",
    "for category in categories:\n",
    "    path = os.path.join(dataset_dir, category)\n",
    "    label = category\n",
    "    \n",
    "    for count, img_name in enumerate(os.listdir(path), start=1):\n",
    "        \n",
    "        img_path = os.path.join(path, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        try:\n",
    "            # Apply data augmentation and extract features\n",
    "            image = cv2.resize(image, (128, 128))  # Resize to a fixed size\n",
    "            \n",
    "            # Extract features from the original image\n",
    "            original_features = extract_features(image)\n",
    "            data.append(original_features)\n",
    "            labels.append(label)\n",
    "            groups.append(group_id)  # Assign the group ID to the original image\n",
    "\n",
    "            # Prepare for augmentation\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            aug_iter = datagen.flow(image, batch_size=1)\n",
    "\n",
    "            # Perform 4 augmentations per image\n",
    "            for _ in range(4):\n",
    "                aug_img = next(aug_iter)[0].astype(np.uint8)\n",
    "                features = extract_features(aug_img)\n",
    "                data.append(features)\n",
    "                labels.append(label)\n",
    "                groups.append(group_id)  # Assign the same group ID to the augmentations\n",
    "\n",
    "            # Increment group ID for the next image and its augmentations\n",
    "            group_id += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {img_path}, {img_name}\")\n",
    "\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Convert lists to NumPy arrays\n",
    "final_data = np.array(data)\n",
    "final_labels = np.array(labels)\n",
    "final_groups = np.array(groups)\n",
    "\n",
    "\n",
    "if not os.path.exists(\"Extracted_features\"):\n",
    "    os.makedirs(\"Extracted_features\")\n",
    "\n",
    "\n",
    "data_mmap = np.memmap('Extracted_features\\\\final_data_mmap.dat', dtype='float32', mode='w+', shape=final_data.shape)\n",
    "labels_mmap = np.memmap('Extracted_features\\\\final_labels_mmap.dat', dtype='int', mode='w+', shape=final_labels.shape)\n",
    "groups_mmap = np.memmap('Extracted_features\\\\final_groups_mmap.dat', dtype='int', mode='w+', shape=final_groups.shape)\n",
    "\n",
    "# Copy data to memory-mapped files (you can do this in batches)\n",
    "data_mmap[:] = final_data[:]\n",
    "labels_mmap[:] = final_labels[:]\n",
    "groups_mmap[:] = final_groups[:]\n",
    "\n",
    "# Flush to disk\n",
    "data_mmap.flush()\n",
    "labels_mmap.flush()\n",
    "groups_mmap.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New CODE FOR DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image: textures 3/corduroy/.ipynb_checkpoints, .ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Initialize batch size for saving\n",
    "batch_size = 1000  # You can adjust this based on memory capacity\n",
    "save_count = 0  # To track saved files\n",
    "\n",
    "# Initialize arrays\n",
    "data = []\n",
    "labels = []\n",
    "groups = []  # This will store the group IDs\n",
    "\n",
    "group_id = 0  # Initialize group ID\n",
    "\n",
    "# Image Augmentation using TensorFlow\n",
    "datagen = ImageDataGenerator(\n",
    "    horizontal_flip=True,     \n",
    "    vertical_flip=True,       \n",
    "    brightness_range=[0.8, 1.2], \n",
    "    shear_range=0.4           \n",
    ")\n",
    "\n",
    "for category in categories:\n",
    "    path = os.path.join(dataset_dir, category)\n",
    "    label = category\n",
    "\n",
    "    for count, img_name in enumerate(os.listdir(path), start=1):\n",
    "        \n",
    "        img_path = os.path.join(path, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        try:\n",
    "            # Apply data augmentation and extract features\n",
    "            image = cv2.resize(image, (128, 128))  # Resize to a fixed size\n",
    "            \n",
    "            # Extract features from the original image\n",
    "            original_features = extract_features(image)\n",
    "            data.append(original_features)\n",
    "            labels.append(label)\n",
    "            groups.append(group_id)  # Assign the group ID to the original image\n",
    "\n",
    "            # Prepare for augmentation\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            aug_iter = datagen.flow(image, batch_size=1)\n",
    "\n",
    "            # Perform 4 augmentations per image\n",
    "            for _ in range(4):\n",
    "                aug_img = next(aug_iter)[0].astype(np.uint8)\n",
    "                features = extract_features(aug_img)\n",
    "                data.append(features)\n",
    "                labels.append(label)\n",
    "                groups.append(group_id)  # Assign the same group ID to the augmentations\n",
    "\n",
    "            # Increment group ID for the next image and its augmentations\n",
    "            group_id += 1\n",
    "\n",
    "            # Save data in batches\n",
    "            if len(data) >= batch_size:\n",
    "                # Save current batch to an .npz file\n",
    "                np.savez_compressed(f'Extracted_features\\\\dataset_batch_{save_count}.npz', data=np.array(data), labels=np.array(labels), groups=np.array(groups))\n",
    "                save_count += 1\n",
    "\n",
    "                # Clear memory by resetting the arrays and forcing garbage collection\n",
    "                data.clear()\n",
    "                labels.clear()\n",
    "                groups.clear()\n",
    "                gc.collect()  # Force garbage collection to free memory\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {img_path}, {img_name}\")\n",
    "\n",
    "# Save any remaining data if exists after loop ends\n",
    "if data:\n",
    "    np.savez_compressed(f'Extracted_features\\\\dataset_batch_{save_count}.npz', data=np.array(data), labels=np.array(labels), groups=np.array(groups))\n",
    "    data.clear()\n",
    "    labels.clear()\n",
    "    groups.clear()\n",
    "    gc.collect()  # Clean up the remaining memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Code (using data,labels and groups directly as ndarrays rather than lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Counting the number of rows(images) and features\n",
    "import os\n",
    "\n",
    "# Initialize counters\n",
    "total_original_images = 0\n",
    "total_augmented_images = 0\n",
    "augmentation_per_image = 4  # As you're doing 4 augmentations per image\n",
    "feature_size = None  # To store the size of features (columns)\n",
    "\n",
    "for category in categories:\n",
    "    path = os.path.join(dataset_dir, category)\n",
    "    \n",
    "    for count, img_name in enumerate(os.listdir(path), start=1):\n",
    "        img_path = os.path.join(path, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        try:\n",
    "            image = cv2.resize(image, (128, 128))  # Resize to a fixed size\n",
    "            original_features = extract_features(image)\n",
    "            \n",
    "            # Determine feature size (columns) from the first image processed\n",
    "            if feature_size is None:\n",
    "                feature_size = original_features.shape[0]  # Assuming 1D feature vector\n",
    "            \n",
    "            # Increment counts for original and augmented images\n",
    "            total_original_images += 1\n",
    "            total_augmented_images += augmentation_per_image\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {img_path}, {img_name}\")\n",
    "\n",
    "# Total number of images = original + augmented\n",
    "total_images = total_original_images + total_augmented_images\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total original images: {total_original_images}\")\n",
    "print(f\"Total augmented images: {total_augmented_images}\")\n",
    "print(f\"Total images including augmentations: {total_images}\")\n",
    "print(f\"Number of features per image: {feature_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Image Augmentation using TensorFlow\n",
    "datagen = ImageDataGenerator(\n",
    "    horizontal_flip=True,     \n",
    "    vertical_flip=True,       \n",
    "    brightness_range=[0.8, 1.2], \n",
    "    shear_range=0.4           \n",
    ")\n",
    "\n",
    "# Step 1: Create \"Extracted_features\" folder if it doesn't exist\n",
    "output_dir = 'Extracted_features'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created directory: {output_dir}\")\n",
    "\n",
    "# Step 2: Define file paths for the memory-mapped arrays inside the new folder\n",
    "data_file = os.path.join(output_dir, 'data_file.dat')\n",
    "labels_file = os.path.join(output_dir, 'labels_file.dat')\n",
    "groups_file = os.path.join(output_dir, 'groups_file.dat')\n",
    "# Known values\n",
    "total_images = 17780  # Total original images + augmentations\n",
    "feature_size = 49162   # Number of features per image\n",
    "\n",
    "# Step 1: Preallocate memory-mapped arrays\n",
    "data = np.memmap(data_file, dtype='float32', mode='w+', shape=(total_images, feature_size))\n",
    "labels = np.memmap(labels_file, dtype='object', mode='w+', shape=(total_images,))\n",
    "groups = np.memmap(groups_file, dtype='int32', mode='w+', shape=(total_images,))\n",
    "\n",
    "# Step 2: Feature Extraction with memory mapping\n",
    "group_id = 0  # Initialize group ID\n",
    "image_counter = 0  # Keep track of which row we are filling in the arrays\n",
    "\n",
    "for category in categories:\n",
    "    path = os.path.join(dataset_dir, category)\n",
    "    label = category\n",
    "    \n",
    "    for count, img_name in enumerate(os.listdir(path), start=1):\n",
    "        img_path = os.path.join(path, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        try:\n",
    "            image = cv2.resize(image, (128, 128))  # Resize to a fixed size\n",
    "            \n",
    "            # Extract features from the original image\n",
    "            original_features = extract_features(image)\n",
    "            \n",
    "            # Store the original image's features, label, and group ID in the memory-mapped arrays\n",
    "            data[image_counter, :] = original_features  # Fill in the row corresponding to the current image\n",
    "            labels[image_counter] = label\n",
    "            groups[image_counter] = group_id\n",
    "            image_counter += 1  # Increment counter to fill the next row\n",
    "\n",
    "            # Prepare for augmentation\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            aug_iter = datagen.flow(image, batch_size=1)\n",
    "\n",
    "            # Perform 4 augmentations per image\n",
    "            for _ in range(4):\n",
    "                aug_img = next(aug_iter)[0].astype(np.uint8)\n",
    "                features = extract_features(aug_img)\n",
    "                \n",
    "                # Store augmented image features, label, and group ID\n",
    "                data[image_counter, :] = features\n",
    "                labels[image_counter] = label\n",
    "                groups[image_counter] = group_id\n",
    "                image_counter += 1  # Increment counter for the next augmented image\n",
    "\n",
    "            # Increment group ID for the next set of images\n",
    "            group_id += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "# Flush the changes to disk to ensure everything is saved\n",
    "data.flush()\n",
    "labels.flush()\n",
    "groups.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Freeing up the memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "del data\n",
    "del labels\n",
    "del groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data,labels and groups**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Memory-mapped file paths\n",
    "output_dir = 'Extracted_features'\n",
    "data_file = os.path.join(output_dir, 'data_file.dat')\n",
    "labels_file = os.path.join(output_dir, 'labels_file.dat')\n",
    "groups_file = os.path.join(output_dir, 'groups_file.dat')\n",
    "\n",
    "# Known values\n",
    "total_images = 17780  # Total original images + augmentations\n",
    "feature_size = 49162   # Number of features per image \n",
    "\n",
    "# Load the memory-mapped arrays for reading\n",
    "data = np.memmap(data_file, dtype='float32', mode='r', shape=(total_images, feature_size))\n",
    "labels = np.memmap(labels_file, dtype='object', mode='r', shape=(total_images,))\n",
    "groups = np.memmap(groups_file, dtype='int32', mode='r', shape=(total_images,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17780, 49162)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data** contains the input data (X) <br>\n",
    "**Labels** contains the output data (Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the data,labels and groups**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Convert lists to NumPy arrays\n",
    "data_np = np.array(data)\n",
    "labels_np = np.array(labels)\n",
    "groups_np = np.array(groups)\n",
    "\n",
    "if not os.path.exists(\"Extracted_features\"):\n",
    "    os.makedirs(\"Extracted_features\")\n",
    "# Save to a .npz file\n",
    "np.savez('Extracted_features\\\\data.npz', data=data_np)\n",
    "np.savez('Extracted_features\\\\labels.npz', labels=labels_np)\n",
    "np.savez('Extracted_features\\\\groups.npz',groups=groups_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Directory where your .npz files are stored\n",
    "npz_directory = './'  # Replace with the correct directory path if necessary\n",
    "\n",
    "# Automatically list all .npz files in the directory\n",
    "npz_files = [f for f in os.listdir(npz_directory) if f.endswith('.npz')]\n",
    "\n",
    "# Initialize empty lists to store the extracted data\n",
    "all_data = []\n",
    "all_labels = []\n",
    "all_groups = []\n",
    "\n",
    "# Iterate through each saved .npz file and load the arrays\n",
    "for file in npz_files:\n",
    "    file_path = os.path.join(npz_directory, file)\n",
    "    # Load the saved .npz file\n",
    "    with np.load(file_path) as data:\n",
    "        all_data.append(data['data'])    # Append 'data' array\n",
    "        all_labels.append(data['labels']) # Append 'labels' array\n",
    "        all_groups.append(data['groups']) # Append 'groups' array\n",
    "\n",
    "# Now concatenate all arrays to create final datasets\n",
    "final_data = np.concatenate(all_data, axis=0)\n",
    "final_labels = np.concatenate(all_labels, axis=0)\n",
    "final_groups = np.concatenate(all_groups, axis=0)\n",
    "\n",
    "# Your final arrays are now ready to use\n",
    "print(\"Final data shape:\", final_data.shape)\n",
    "print(\"Final labels shape:\", final_labels.shape)\n",
    "print(\"Final groups shape:\", final_groups.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------End----------------------------------\n",
    "\n",
    "MOVE TO model_training.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the saved data and labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from the .npz file\n",
    "loaded_data = np.load('Extracted_features\\\\data.npz')\n",
    "loaded_labels = np.load('Extracted_features\\\\labels.npz')\n",
    "loaded_groups = np.load('Extracted_features\\\\groups.npz')\n",
    "\n",
    "data_loaded = loaded_data[\"data\"]\n",
    "labels_loaded = loaded_labels[\"labels\"]\n",
    "groups_loaded = loaded_labels[\"groups\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (17780, 49162)\n",
      "Labels shape: (17780,)\n"
     ]
    }
   ],
   "source": [
    "# Checking the shape of the dataset and the labels\n",
    "print(f\"Dataset shape: {data_loaded.shape}\")\n",
    "print(f\"Labels shape: {labels_loaded.shape}\")\n",
    "print(f\"Groups shape: {groups_loaded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output we can see that we have **49162 features** and **17780 rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_loaded\n",
    "labels = labels_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (12446, 49162)\n",
      "X_test shape:  (5334, 49162)\n",
      "y_train shape:  (12446,)\n",
      "y_test shape:  (5334,)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(data), np.array(labels), test_size=0.3, random_state=42)\n",
    "print(\"X_train shape: \",X_train.shape)\n",
    "print(\"X_test shape: \",X_test.shape)\n",
    "print(\"y_train shape: \",y_train.shape)\n",
    "print(\"y_test shape: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction using PCA(Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction using PCA \n",
    "pca = PCA(n_components=1000)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Random Forest Classifier \n",
    "random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC(random_state=42)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply SVM model (using RBF kernel )\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    corduroy       0.93      0.68      0.79      1055\n",
      "      cotton       0.65      0.89      0.75      1062\n",
      "       denim       0.80      0.94      0.86      1092\n",
      "       linin       0.83      0.61      0.70      1091\n",
      "        wool       0.65      0.63      0.64      1034\n",
      "\n",
      "    accuracy                           0.75      5334\n",
      "   macro avg       0.77      0.75      0.75      5334\n",
      "weighted avg       0.77      0.75      0.75      5334\n",
      "\n",
      "[[ 718   99   78   41  119]\n",
      " [   8  946   21   31   56]\n",
      " [   0    2 1023    7   60]\n",
      " [  21  235   46  668  121]\n",
      " [  26  180  114   61  653]]\n"
     ]
    }
   ],
   "source": [
    "# Predictions and Evaluation\n",
    "y_pred = random_forest_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    corduroy       0.52      0.73      0.61      1055\n",
      "      cotton       0.43      0.56      0.48      1062\n",
      "       denim       0.96      0.82      0.88      1092\n",
      "       linin       0.60      0.42      0.49      1091\n",
      "        wool       0.63      0.48      0.55      1034\n",
      "\n",
      "    accuracy                           0.60      5334\n",
      "   macro avg       0.63      0.60      0.60      5334\n",
      "weighted avg       0.63      0.60      0.61      5334\n",
      "\n",
      "[[766 163  11  50  65]\n",
      " [240 592   5 158  67]\n",
      " [ 79   8 896  30  79]\n",
      " [147 395   7 457  85]\n",
      " [228 225  15  65 501]]\n"
     ]
    }
   ],
   "source": [
    "# Predictions and Evaluation\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking class imbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linin       3585\n",
      "denim       3575\n",
      "corduroy    3550\n",
      "cotton      3535\n",
      "wool        3535\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(labels_loaded).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pd.Series(labels_loaded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((pd.Series(labels_loaded).value_counts() / len(pd.Series(labels_loaded)))*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
