{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dump\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from joblib import dump\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dask_ml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcudf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcuml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdcv\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcuml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcunl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dask_ml'"
     ]
    }
   ],
   "source": [
    "# CUDA libraries\n",
    "import cupy as cp \n",
    "import cudf\n",
    "from cuml.preprocessing import StandardScaler\n",
    "import dask_ml.model_selection as dcv\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "from cunl.svm import SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the saved data and labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from the .npz files\n",
    "\n",
    "loaded_data = np.load('Extracted_features\\\\data.npz')\n",
    "loaded_labels = np.load('Extracted_features\\\\labels.npz')\n",
    "loaded_groups = np.load('Extracted_features\\\\groups.npz')\n",
    "\n",
    "X = loaded_data[\"data\"]\n",
    "y = loaded_labels[\"labels\"]\n",
    "groups = loaded_groups[\"groups\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of X <class 'numpy.ndarray'>\n",
      "Type of y <class 'numpy.ndarray'>\n",
      "Type of groups <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Checking the data type of X,y and groups\n",
    "print(f\"Type of X {type(X)}\")\n",
    "print(f\"Type of y {type(y)}\")\n",
    "print(f\"Type of groups {type(groups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (17780, 49162)\n",
      "Labels shape: (17780,)\n",
      "Groups shape: (17780,)\n"
     ]
    }
   ],
   "source": [
    "# Checking the shape of the dataset and the labels\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Groups shape: {groups.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cotton' 'cotton' 'cotton' ... 'wool' 'wool' 'wool']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding the target variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 5 5 5]\n",
      "(17780,)\n"
     ]
    }
   ],
   "source": [
    "# Example array with categories\n",
    "categories = ['linen', 'cotton', 'wool', 'denim', 'corduroy']\n",
    "\n",
    "# Create a dictionary for manual mapping\n",
    "category_mapping = { 'corduroy': 1, 'cotton': 2, 'denim': 3, 'linin': 4, 'wool': 5}\n",
    "\n",
    "# Convert to pandas Series (optional if already in pandas)\n",
    "y_series = pd.Series(y)\n",
    "\n",
    "# Map categories to numbers\n",
    "mapped_categories = y_series.map(category_mapping)\n",
    "y = np.array(mapped_categories)\n",
    "\n",
    "\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linin       3585\n",
      "denim       3575\n",
      "corduroy    3550\n",
      "cotton      3535\n",
      "wool        3535\n",
      "Name: count, dtype: int64\n",
      "4    3585\n",
      "3    3575\n",
      "1    3550\n",
      "2    3535\n",
      "5    3535\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_series.value_counts())\n",
    "print(mapped_categories.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\cupy\\_creation\\from_data.py:53: PerformanceWarning: Using synchronous transfer as pinned memory (6992802880 bytes) could not be allocated. This generally occurs because of insufficient host memory. The original error was: cudaErrorMemoryAllocation: out of memory\n",
      "  return _core.array(obj, dtype, copy, order, subok, ndmin, blocking)\n"
     ]
    }
   ],
   "source": [
    "# Since X,y and groups are numpy arrays , let's convert them to cupy arrays\n",
    "X = cp.array(X)\n",
    "y = cp.array(y)\n",
    "groups= cp.array(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of X <class 'cupy.ndarray'>\n",
      "Type of y <class 'cupy.ndarray'>\n",
      "Type of groups <class 'cupy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Checking the data type of X,y and groups\n",
    "print(f\"Type of X {type(X)}\")\n",
    "print(f\"Type of y {type(y)}\")\n",
    "print(f\"Type of groups {type(groups)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Splitting the original dataset into train(80%) and test(20%) using GroupShuffleSplit to ensure that the same group of images are either in the train or the test dataset only.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "Out of memory allocating 5,592,669,184 bytes (allocated so far: 6,993,131,008 bytes).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m gss \u001b[38;5;241m=\u001b[39m GroupShuffleSplit(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      2\u001b[0m train_idx, test_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(gss\u001b[38;5;241m.\u001b[39msplit(cp\u001b[38;5;241m.\u001b[39masnumpy(X), cp\u001b[38;5;241m.\u001b[39masnumpy(y), groups\u001b[38;5;241m=\u001b[39mcp\u001b[38;5;241m.\u001b[39masnumpy(groups)))\n\u001b[1;32m----> 4\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m, X[test_idx]\n\u001b[0;32m      5\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m y[train_idx], y[test_idx]\n\u001b[0;32m      6\u001b[0m groups_train \u001b[38;5;241m=\u001b[39m groups[train_idx]\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:1527\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.__getitem__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\_routines_indexing.pyx:43\u001b[0m, in \u001b[0;36mcupy._core._routines_indexing._ndarray_getitem\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:835\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.take\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\_routines_indexing.pyx:132\u001b[0m, in \u001b[0;36mcupy._core._routines_indexing._ndarray_take\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\_routines_indexing.pyx:816\u001b[0m, in \u001b[0;36mcupy._core._routines_indexing._take\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:137\u001b[0m, in \u001b[0;36mcupy._core.core.ndarray.__new__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:225\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base._init\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:738\u001b[0m, in \u001b[0;36mcupy.cuda.memory.alloc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:1424\u001b[0m, in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:1445\u001b[0m, in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:1116\u001b[0m, in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool.malloc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:1137\u001b[0m, in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._malloc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:1382\u001b[0m, in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._try_malloc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\cuda\\memory.pyx:1385\u001b[0m, in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._try_malloc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: Out of memory allocating 5,592,669,184 bytes (allocated so far: 6,993,131,008 bytes)."
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(cp.asnumpy(X), cp.asnumpy(y), groups=cp.asnumpy(groups)))\n",
    "\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "groups_train = groups[train_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested cross-validation setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define Outer Loop: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_cv = GroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Training Data into Outer Training and Validation Folds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.78 GiB for an array with shape (7585, 49162) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"e:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"e:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"e:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n  File \"e:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\joblib\\parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"e:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 136, in __call__\n    return self.function(*args, **kwargs)\n  File \"e:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 880, in _fit_and_score\n    X_train, y_train = _safe_split(estimator, X, y, train)\n  File \"e:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 156, in _safe_split\n    X_subset = _safe_indexing(X, indices)\n  File \"e:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\sklearn\\utils\\_indexing.py\", line 267, in _safe_indexing\n    return _array_indexing(X, indices, indices_dtype, axis=axis)\n  File \"e:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\sklearn\\utils\\_indexing.py\", line 33, in _array_indexing\n    return array[key, ...] if axis == 0 else array[:, key]\n  File \"e:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\numpy\\core\\memmap.py\", line 335, in __getitem__\n    res = super().__getitem__(index)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 2.78 GiB for an array with shape (7585, 49162) and data type float64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m X_outer_val \u001b[38;5;241m=\u001b[39m X_train[outer_val_idx] \u001b[38;5;66;03m# Validation features\u001b[39;00m\n\u001b[0;32m     14\u001b[0m y_outer_val \u001b[38;5;241m=\u001b[39m y_train[outer_val_idx] \u001b[38;5;66;03m# Validation labels\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m svm_model, rf_model \u001b[38;5;241m=\u001b[39m \u001b[43mstart_inner_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[26], line 11\u001b[0m, in \u001b[0;36mstart_inner_loop\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m param_grid_svm, param_grid_rf \u001b[38;5;241m=\u001b[39m create_param_grids()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Performing RandomSearchCV for SVM and RF\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m random_search_svm \u001b[38;5;241m=\u001b[39m \u001b[43mrandomized_search_svm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_svm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparam_grid_svm\u001b[49m\u001b[43m,\u001b[49m\u001b[43minner_cv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m random_search_rf \u001b[38;5;241m=\u001b[39m randomized_search_rf(pipeline_rf,param_grid_rf,inner_cv)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Get the best models of SVM and RF\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 13\u001b[0m, in \u001b[0;36mrandomized_search_svm\u001b[1;34m(pipeline_svm, param_grid_svm, inner_cv)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandomized_search_svm\u001b[39m(pipeline_svm,param_grid_svm,inner_cv):\n\u001b[0;32m      4\u001b[0m     random_search_svm \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m      5\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mpipeline_svm,\n\u001b[0;32m      6\u001b[0m         param_distributions\u001b[38;5;241m=\u001b[39mparam_grid_svm,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# Uses all processors\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     )\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mrandom_search_svm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_outer_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_outer_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m random_search_svm\n",
      "File \u001b[1;32me:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32me:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1960\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1959\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1960\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1962\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    963\u001b[0m     )\n\u001b[1;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m     )\n",
      "File \u001b[1;32me:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32me:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32me:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Projects\\Fabric_Detection\\fabric_venv\\lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.78 GiB for an array with shape (7585, 49162) and data type float64"
     ]
    }
   ],
   "source": [
    "# outer_train_idx: Indices for the training data in the current fold\n",
    "# outer_val_idx: Indices for the testing data in the current fold\n",
    "\n",
    "# The following loops runs 5 times as the no of splits in outer_cv has been defined as 5\n",
    "count=1\n",
    "for outer_train_idx, outer_val_idx in outer_cv.split(cp.asnumpy(X_train), cp.asnumpy(y_train), groups=cp.asnumpy(groups_train)):\n",
    "    # Train data for the current fold\n",
    "    X_outer_train = X_train[outer_train_idx] # Training features\n",
    "    y_outer_train = y_train[outer_train_idx] # Training lables\n",
    "    groups_outer_train = groups_train[outer_train_idx] # Training groups\n",
    "\n",
    "    # Validation data for the current fold\n",
    "    X_outer_val = X_train[outer_val_idx] # Validation features\n",
    "    y_outer_val = y_train[outer_val_idx] # Validation labels\n",
    "\n",
    "    svm_model, rf_model = start_inner_loop()\n",
    "\n",
    "    if not os.path.exists(\"Models\"):\n",
    "        os.makedirs(\"Models\")\n",
    "\n",
    "    dump(svm_model, f'Models\\\\svm_{count}.joblib')\n",
    "    dump(rf_model,f\"Models\\\\rf_{count}.joblib\")\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Loop: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Inner Cross-Validation Loop**\n",
    "\n",
    "- Using GroupKFold: For hyperparameter tuning within the outer training fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_inner_loop():\n",
    "\n",
    "    # Defining inner cross-validation strategy\n",
    "    inner_cv = GroupKFold(n_splits=3)\n",
    "\n",
    "    # Creating pipelines and parameters grids for SVM and RF models\n",
    "    pipeline_svm, pipeline_rf = create_pipelines()\n",
    "    param_grid_svm, param_grid_rf = create_param_grids()\n",
    "\n",
    "    # Performing RandomSearchCV for SVM and RF\n",
    "    random_search_svm = randomized_search_svm(pipeline_svm,param_grid_svm,inner_cv)\n",
    "    random_search_rf = randomized_search_rf(pipeline_rf,param_grid_rf,inner_cv)\n",
    "\n",
    "    # Get the best models of SVM and RF\n",
    "    best_model_svm = get_best_model_svm(random_search_svm)\n",
    "    best_model_rf = get_best_model_rf(random_search_rf)\n",
    "\n",
    "    evaluate_best_model_svm(best_model_svm)\n",
    "    evaluate_best_model_rf(best_model_rf)\n",
    "\n",
    "    return best_model_svm, best_model_rf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Pipelines Including Preprocessing and Classifier**\n",
    "- For SVM and Random Forest separately\n",
    "- Include Preprocessing Steps:\n",
    "   - Scaling (StandardScaler)\n",
    "   - Principal Component Analysis (with number of components as hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for Support Vector Machine Classifier\n",
    "\n",
    "def create_pipelines():\n",
    "    pipeline_svm = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\",PCA()),\n",
    "        (\"classifier\", SVC())\n",
    "    ])\n",
    "\n",
    "    # Pipeline for Random Forest Classifier\n",
    "    pipeline_rf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\",PCA()),\n",
    "        (\"classifier\", RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    return pipeline_svm, pipeline_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Hyperparameter grids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for SVM\n",
    "def create_param_grids():\n",
    "    param_grid_svm = {\n",
    "        'pca__n_components': [1000,2000,3000],\n",
    "        'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__kernel': ['rbf'],\n",
    "        'classifier__gamma': ['scale', 'auto']\n",
    "    }\n",
    "\n",
    "\n",
    "    # Parameter grid for Random Forest Classifier\n",
    "    param_grid_rf = {\n",
    "        'pca__n_components': [1000,2000,3000],\n",
    "        'classifier__n_estimators': [100, 300],\n",
    "        'classifier__max_depth': [10, 20]\n",
    "    }\n",
    "\n",
    "    return param_grid_svm, param_grid_rf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform RandomizedSearch with Cross-validation for Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomziedSearchCV for SVM\n",
    "\n",
    "def randomized_search_svm(pipeline_svm,param_grid_svm,inner_cv):\n",
    "    random_search_svm = dcv.RandomizedSearchCV(\n",
    "        estimator=pipeline_svm,\n",
    "        param_distributions=param_grid_svm,\n",
    "        cv = inner_cv.split(cp.asnumpy(X_outer_train),cp.asnumpy(y_outer_train), groups=cp.asnumpy(groups_outer_train)),\n",
    "        scoring=\"f1_macro\",\n",
    "        n_iter=5,\n",
    "        n_jobs=-1 # Uses all processors\n",
    "    )\n",
    "\n",
    "    random_search_svm.fit(X_outer_train, y_outer_train)\n",
    "\n",
    "    return random_search_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_svm(random_search_svm):\n",
    "    best_model_svm = random_search_svm.best_estimator_\n",
    "    best_params_svm = random_search_svm.best_params_\n",
    "    best_scores_svm = random_search_svm.best_score_\n",
    "\n",
    "    print(f\"Best params for SVM Model: {best_params_svm}\")\n",
    "    print(f\"Best scoring(F1) for SVM Model: {best_scores_svm}\")\n",
    "\n",
    "    return best_model_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomziedSearchCV for Random Forest Classifier\n",
    "def randomized_search_rf(pipeline_rf, param_grid_rf, inner_cv):\n",
    "    random_search_rf = dcv.RandomizedSearchCV(\n",
    "        estimator=pipeline_rf,\n",
    "        param_distributions=param_grid_rf,\n",
    "        cv = inner_cv.split(cp.asnumpy(X_outer_train),cp.asnumpy(y_outer_train), groups=cp.asnumpy(groups_outer_train)),\n",
    "        scoring=\"f1_macro\",\n",
    "        n_iter=5,\n",
    "        n_jobs=-1 # Uses all processors\n",
    "    )\n",
    "\n",
    "    random_search_rf.fit(X_outer_train, y_outer_train)\n",
    "\n",
    "    return random_search_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_rf(random_search_rf):\n",
    "    best_model_rf = random_search_rf.best_estimator_\n",
    "    best_params_rf = random_search_rf.best_params_\n",
    "    best_scores_rf = random_search_rf.best_score_\n",
    "\n",
    "    print(f\"Best params for RF Model: {best_params_rf}\")\n",
    "    print(f\"Best scoring(F1) for RF Model: {best_scores_rf}\")\n",
    "\n",
    "    return best_model_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Best Model from RandomizedSearch on Outer Validation Fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SVM Model\n",
    "def evaluate_best_model_svm(best_model_svm):\n",
    "    y_outer_val_pred_svm = best_model_svm.predict(X_outer_val)\n",
    "    print(classification_report(y_outer_val,y_outer_val_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For RF Model\n",
    "def evaluate_best_model_rf(best_model_rf):\n",
    "    y_outer_val_pred_rf = best_model_rf.predict(X_outer_val)\n",
    "    print(classification_report(y_outer_val,y_outer_val_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEPS 9 ARE PENDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
