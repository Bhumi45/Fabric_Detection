{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we tried to effective memory efficient way of encoding our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from joblib import dump\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import cv2\n",
    "import pandas as pd\n",
    "#import cupy as cp  # CuPy for GPU-based NumPy operations\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "import scipy\n",
    "#from skimage.feature import local_binary_pattern\n",
    "#from skimage.filters import gabor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data,labels and groups**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-mapped file paths\n",
    "output_dir = 'Extracted_features'\n",
    "data_file = os.path.join(output_dir, 'data_file.dat')\n",
    "labels_file = os.path.join(output_dir, 'labels_file.dat')\n",
    "labels_encoded_file = os.path.join(output_dir,\"labels_encoded.dat\")\n",
    "groups_file = os.path.join(output_dir, 'groups_file.dat')\n",
    "\n",
    "# Known values\n",
    "total_images = 17780  # Total original images + augmentations\n",
    "feature_size = 49162   # Number of features per image \n",
    "\n",
    "# Load the memory-mapped arrays for reading\n",
    "data = np.memmap(data_file, dtype='float32', mode='r', shape=(total_images, feature_size))\n",
    "labels = np.memmap(labels_file, dtype='object', mode='r', shape=(total_images,))\n",
    "labels_encoded = np.memmap(labels_encoded_file, dtype='int32', mode='w+', shape=(total_images,))\n",
    "groups = np.memmap(groups_file, dtype='int32', mode='r', shape=(total_images,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17780, 49162)\n",
      "(17780,)\n",
      "(17780,)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Checking the shapes of data,labels and groups\n",
    "print(data.shape)\n",
    "print(labels.shape)\n",
    "print(groups.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "49162 features and 17780 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label Encoding of target variable**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "labels_encoded[:] = label_encoder.fit_transform(labels.astype(str))  # Convert object labels to string before encoding\n",
    "\n",
    "# Step 4: Flush changes to disk\n",
    "labels_encoded.flush()\n",
    "\n",
    "# Optionally delete the old labels array if you no longer need it\n",
    "del labels\n",
    "\n",
    "print(\"Label encoding completed and stored in new memory-mapped array.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit([\"corduroy\",\"cotton\",\"denim\",\"wool\",\"linin\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "# Step 2: Initialize label encoder and determine chunk size\n",
    "#label_encoder = LabelEncoder()\n",
    "chunk_size = 1000  # Choose a chunk size that fits comfortably in memory (adjust if needed)\n",
    "\n",
    "# Step 3: Process the labels in chunks\n",
    "for start in range(0, len(labels), chunk_size):\n",
    "    end = min(start + chunk_size, len(labels))\n",
    "    print(start,end)\n",
    "    # Read a chunk of labels\n",
    "    label_chunk = labels[start:end]  # Convert to string\n",
    "    \n",
    "    # Encode the chunk\n",
    "    encoded_chunk = label_encoder.transform(label_chunk)\n",
    "    \n",
    "    # Write the encoded chunk to the new memory-mapped array\n",
    "    #labels_encoded[start:end] = encoded_chunk\n",
    "    \n",
    "    # Optionally flush after each chunk to ensure data is written to disk\n",
    "    #labels_encoded.flush()\n",
    "\n",
    "# Step 4: Final flush and cleanup\n",
    "#labels_encoded.flush()\n",
    "del labels\n",
    "\n",
    "print(\"Label encoding completed and stored in new memory-mapped array in chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Create a dictionary mapping unique labels to integers\n",
    "unique_labels = np.unique(labels)\n",
    "label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "# Step 2: Process labels in chunks and encode using the dictionary\n",
    "chunk_size = 1000\n",
    "\n",
    "for start in range(0, len(labels), chunk_size):\n",
    "    end = min(start + chunk_size, len(labels))\n",
    "    \n",
    "    label_chunk = labels[start:end].astype(str)  # Convert to string\n",
    "    encoded_chunk = np.array([label_map[label] for label in label_chunk], dtype=np.int32)\n",
    "    \n",
    "    labels_encoded[start:end] = encoded_chunk\n",
    "    labels_encoded.flush()\n",
    "\n",
    "print(\"Label encoding completed using dictionary mapping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
